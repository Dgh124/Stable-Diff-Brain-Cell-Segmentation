{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1UYj9fNg_Gm","executionInfo":{"status":"ok","timestamp":1737873346585,"user_tz":300,"elapsed":27679,"user":{"displayName":"Dahirou Dabo Harden","userId":"03178038846286108204"}},"outputId":"0ee547f7-bc4d-46a7-e546-3bfa219eefd4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["INIT"],"metadata":{"id":"sXmVPrTVjQee"}},{"cell_type":"code","source":["import os\n","import torch\n","\n","# Specify the path you want to change to\n","new_directory = \"/content/drive/MyDrive/Stable_diff\"\n","\n","# Change the current working directory\n","os.chdir(new_directory)\n","\n","# Confirm the directory has changed\n","print(\"Current Directory:\", os.getcwd())\n","\n","DEVICE = \"cpu\"\n","\n","ALLOW_CUDA = True\n","ALLOW_MPS = True\n","\n","if torch.cuda.is_available() and ALLOW_CUDA:\n","    DEVICE = \"cuda\"\n","elif (torch.backends.mps.is_available()) and ALLOW_MPS:\n","    DEVICE = \"mps\"\n","print(f\"Using device: {DEVICE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHiaOIolinQi","executionInfo":{"status":"ok","timestamp":1737873349831,"user_tz":300,"elapsed":3249,"user":{"displayName":"Dahirou Dabo Harden","userId":"03178038846286108204"}},"outputId":"e875222e-979a-4308-dacb-b41c9d3bc53e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Directory: /content/drive/MyDrive/Stable_diff\n","Using device: cuda\n"]}]},{"cell_type":"code","source":["def load_model(model, save_dir, device):\n","    \"\"\"\n","    Load model weights from the specified directory.\n","    Args:\n","        models (dict): Dictionary of model instances.\n","        save_dir (str): Directory containing saved model weights.\n","        device (str): Device to load models onto.\n","    \"\"\"\n","    for model_name, model in models.items():\n","        model_path = os.path.join(save_dir, f\"{model_name}.pt\")\n","        if os.path.exists(model_path):\n","            model.load_state_dict(torch.load(model_path, map_location=device))\n","            print(f\"Loaded {model_name} from {model_path}\")\n","        else:\n","            print(f\"No saved weights found for {model_name} at {model_path}\")\n","    return models"],"metadata":{"id":"Jcm-GV12hV3J","executionInfo":{"status":"ok","timestamp":1737873349831,"user_tz":300,"elapsed":4,"user":{"displayName":"Dahirou Dabo Harden","userId":"03178038846286108204"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"AjeTdEiQZphR","outputId":"74941681-5300-4c51-9a6a-a3cfda9df8e5","executionInfo":{"status":"error","timestamp":1737873410148,"user_tz":300,"elapsed":60320,"user":{"displayName":"Dahirou Dabo Harden","userId":"03178038846286108204"}}},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"VAE_Decoder.__init__() missing 3 required positional arguments: 'stg1_res', 'stg2_res', and 'stg3_res'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-cffaa9e36528>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# models = load_models(models, models_path_dem, DEVICE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mVAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Stable_diff/vae_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE_Encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE_Decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: VAE_Decoder.__init__() missing 3 required positional arguments: 'stg1_res', 'stg2_res', and 'stg3_res'"]}],"source":["import random\n","\n","from encoder import VAE_Encoder\n","from decoder import VAE_Decoder\n","from diffusion import Diffusion\n","from torch import nn, optim\n","from transformers import CLIPTokenizer\n","import torchvision.transforms as transforms\n","from vae_model import VAE\n","\n","import attention\n","import ddpm\n","import pipeline\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, Subset\n","\n","import model_loader\n","from pipeline import train_model\n","\n","from PIL import Image\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.image_filenames = sorted(os.listdir(image_dir))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","    def __getitem__(self, idx):\n","        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n","        # Convert to grayscale ('L' mode)\n","        image = Image.open(image_path).convert('L')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","HEIGHT, WIDTH = 256, 256\n","\n","transform = transforms.Compose([\n","    transforms.Resize((HEIGHT, WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] for 1 channel\n","])\n","\n","pwd = os.getcwd()\n","dataset = ImageDataset(image_dir=f'/content/drive/MyDrive/Stable_diff/complete_tdata/expanded_dataset', transform=transform)\n","\n","batch_size = 2\n","learning_rate = 1e-4\n","momentum = 0.9\n","num_epochs = 25\n","n_timesteps = 1000  # num diffusion steps\n","\n","dataset_size = len(dataset) // 3\n","train_size = int(0.6*dataset_size)\n","val_size = dataset_size - train_size\n","\n","indices = random.sample(range(len(dataset)), dataset_size)\n","small_dataset = Subset(dataset, indices)\n","dataloader = DataLoader(small_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","indices = list(range(dataset_size))\n","random.shuffle(indices)\n","\n","train_sampler = SubsetRandomSampler(indices[:train_size])\n","val_sampler = SubsetRandomSampler(indices[train_size:])\n","\n","train_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n","val_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=2)\n","\n","\n","vae_path = '/content/drive/MyDrive/Stable_diff/train_vae/models/best_model-epoch:6-loss:0.348914.pth'\n","# models = load_models(models, models_path_dem, DEVICE)\n","\n","VAE = VAE().to(DEVICE)\n","\n","VAE.load_state_dict(torch.load(vae_path, map_location=DEVICE))\n","\n","encoder = VAE_Encoder()\n","decoder = VAE_Decoder()\n","\n","encoder.load_state_dict(VAE.encoder.state_dict())\n","decoder.load_state_dict(VAE.decoder.state_dict())\n","\n","# model_file = \"/content/drive/MyDrive/Stable_diff/models/v1-5-pruned-emaonly.ckpt\"\n","# models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n","\n","diffusion = Diffusion()\n","\n","# diff_path = '/content/drive/MyDrive/Stable_diff/models/sd_models/best_diff_model-epoch:1-loss:0.010581.pth'\n","diff_path = '/content/drive/MyDrive/Stable_diff/complete_tdata/overfitt_debug/best_diff_model-epoch:400-loss:5.783495_NG.pth'\n","# diffusion.load_state_dict(torch.load(diff_path, map_location=DEVICE))\n","\n","models = {\n","    'encoder': encoder,\n","    'decoder': decoder,\n","    'diffusion': diffusion\n","}\n","\n","for model in models.values():\n","    model.to(DEVICE)\n","\n","params = []\n","for model in models.values():\n","    params += list(model.parameters())\n","\n","optimizer = optim.SGD(\n","    [\n","        {'params': encoder.parameters()},\n","        {'params': decoder.parameters()},\n","        {'params': diffusion.parameters()}\n","    ],\n","    lr=learning_rate,\n","    momentum=momentum\n",")\n","\n","criterion = torch.nn.MSELoss(reduction='mean')\n","\n","train_model(\n","    models=models,\n","    train_loader=train_dataloader,\n","    val_loader=val_dataloader,\n","    optimizer=optimizer,\n","    criterion=criterion,\n","    n_epochs=num_epochs,\n","    num_timesteps=n_timesteps,\n","    device=DEVICE,\n","    accumulation_steps=6,\n","    mini_patience=3,\n","    full_patience=10,\n",")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"ve6Mex-af7Oz"}},{"cell_type":"code","source":["import model_loader\n","import pipeline\n","from PIL import Image\n","from torchvision.transforms.functional import to_pil_image\n","from pathlib import Path\n","import torch\n","from encoder import VAE_Encoder\n","from decoder import VAE_Decoder\n","from vae_model import VAE\n","from diffusion import Diffusion\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","import torchvision.transforms as transforms\n","import random\n","\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.image_filenames = sorted(os.listdir(image_dir))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","    def __getitem__(self, idx):\n","        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n","        # Convert to grayscale ('L' mode)\n","        image = Image.open(image_path).convert('L')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image\n","\n","HEIGHT, WIDTH = 256, 256\n","\n","transform = transforms.Compose([\n","    transforms.Resize((HEIGHT, WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] for 1 channel\n","])\n","\n","\n","device = \"cpu\"\n","\n","ALLOW_CUDA = True\n","ALLOW_MPS = True\n","\n","if torch.cuda.is_available() and ALLOW_CUDA:\n","    device = \"cuda\"\n","\n","print(f\"Using device: {device}\")\n","\n","\n","# diff_file = \"/content/drive/MyDrive/Stable_diff/models/sd_models/best_diff_model-epoch:7-loss:0.001274.pth\"\n","diff_file = \"/content/drive/MyDrive/Stable_diff/models/sd_models/best_diff_model-epoch:11-loss:0.084444.pth\"\n","\n","# models = model_loader.preload_models_from_standard_weights(model_file, device)\n","\n","diffusion = Diffusion().to(device)\n","diffusion.load_state_dict(torch.load(diff_file, map_location=device))\n","\n","vae_path = '/content/drive/MyDrive/Stable_diff/models/best_model-epoch:5-loss:0.003076.pth'\n","# diffusion = load_model(diffusion, diff_file, DEVICE)\n","##############################\n","VAE = VAE().to(device)\n","\n","VAE.load_state_dict(torch.load(vae_path, map_location=device))\n","\n","encoder = VAE_Encoder()\n","decoder = VAE_Decoder()\n","\n","enc_path = '/content/drive/MyDrive/Stable_diff/models/sd_models/best_enc_model-epoch:1-loss:0.132742.pth'\n","dec_path = '/content/drive/MyDrive/Stable_diff/models/sd_models/best_dec_model-epoch:1-loss:0.132742.pth'\n","\n","encoder.load_state_dict(VAE.encoder.state_dict())\n","decoder.load_state_dict(VAE.decoder.state_dict())\n","\n","# encoder.load_state_dict(torch.load(enc_path, map_location=device))\n","# decoder.load_state_dict(torch.load(dec_path, map_location=device))\n","\n","models = {\n","    'encoder': encoder,\n","    'decoder': decoder,\n","    'diffusion': diffusion\n","}\n","##################################\n","# ## IMAGE TO IMAGE\n","\n","# Comment to disable image to image\n","dataset = ImageDataset(image_dir=f'/content/drive/MyDrive/Stable_diff/complete_tdata/expanded_dataset', transform=transform)\n","# image_path = \"/content/drive/MyDrive/Stable_diff/complete_tdata/overfitt_debug/wop/image_071_tile_41.png\"\n","# input_image = Image.open(image_path)\n","\n","random_index = random.randint(0, len(dataset) - 1)\n","input_image = dataset[random_index]\n","\n","# Higher values means more noise will be added to the input image, so the result will further from the input image.\n","# Lower values means less noise is added to the input image, so output will be closer to the input image.\n","strength = 1.0\n","\n","input_image = None\n","\n","## SAMPLER\n","\n","sampler = \"ddpm\"\n","num_inference_steps = 500\n","seed = 42\n","\n","output_image = pipeline.generate(\n","    input_image=input_image,\n","    strength=strength,\n","    sampler_name=sampler,\n","    n_inference_steps=num_inference_steps,\n","    seed=seed,\n","    models=models,\n","    device=device,\n","    idle_device=\"cpu\"\n",")\n","\n","# Combine the input image and the output image into a single image.\n","# output_image = output_image.astype(np.uint8)\n","# output_image = np.squeeze(output_image)\n","# Image.fromarray(output_image)\n","\n","\n","import matplotlib.pyplot as plt\n","\n","# 1) Convert input_image (a (1,H,W) in [-1,1]) to [0..255] for display\n","input_image_np = input_image.cpu().numpy()  # shape (1,H,W)\n","# scale from [-1..1] -> [0..255]\n","input_image_np = np.clip((input_image_np + 1.0) * 127.5, 0, 255).astype(np.uint8)\n","# remove channel dimension\n","input_image_np = np.squeeze(input_image_np, axis=0)  # shape (H,W)\n","\n","# 2) Output image is already uint8 [0..255], but ensure shape is (H,W)\n","output_image = output_image.astype(np.uint8)\n","output_image = np.squeeze(output_image)\n","\n","# 3) Display side by side with matplotlib\n","plt.figure(figsize=(10, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(input_image_np, cmap='gray')\n","plt.title(\"Input Image\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(output_image, cmap='gray')\n","plt.title(\"Generated Image\")\n","plt.axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_RCgjrXkejyv","colab":{"base_uri":"https://localhost:8080/","height":492},"outputId":"6e1d1dc5-dcab-4237-9fa4-66cb7459cf6a","executionInfo":{"status":"error","timestamp":1736722529057,"user_tz":300,"elapsed":40524,"user":{"displayName":"Dahirou Dabo Harden","userId":"03178038846286108204"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-6dbc0d00b1db>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  diffusion.load_state_dict(torch.load(diff_file, map_location=device))\n","<ipython-input-5-6dbc0d00b1db>:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  VAE.load_state_dict(torch.load(vae_path, map_location=device))\n","100%|██████████| 500/500 [00:23<00:00, 21.31it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Given groups=1, weight of size [256, 544, 1, 1], expected input[1, 32, 32, 32] to have 544 channels, but got 32 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6dbc0d00b1db>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m output_image = pipeline.generate(\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0minput_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mstrength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Stable_diff/pipeline.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(input_image, strength, sampler_name, n_inference_steps, models, seed, device, idle_device)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"decoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mto_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Stable_diff/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoded_features)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# print(f\"\\nX.SHAPE: {x.shape}\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstg1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoded_features\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 544, 1, 1], expected input[1, 32, 32, 32] to have 544 channels, but got 32 channels instead"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}